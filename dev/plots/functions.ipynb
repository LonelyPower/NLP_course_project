{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "workplace_root = r'H:\\My Code\\nlp\\nlp_course_project-master\\news_cls_recm'\n",
    "sys.path.append(workplace_root)\n",
    "sys.path.append(os.path.join(workplace_root, 'dev'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读入数据并预处理及Token化处理\n",
    "\n",
    "实测NLTK加Jieba分词与Tokenize的速度太慢了，后续还是继续使用BERT，在分类模型读取过程中直接使用，不反复读入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ///////\n",
    "import json\n",
    "import jieba\n",
    "import random\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, r'H:\\My Code\\nlp\\nlp_course_project-master\\news_cls_recm\\dev')  # 添加当前目录到搜索路径的最前面\n",
    "\n",
    "from tqdm import tqdm\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from collections import defaultdict\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "from utils import read_and_sample_single_folder\n",
    "from utils import CustomDataset\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# ///\n",
    "bert_path = r'H:\\My Code\\nlp\\nlp_course_project-master\\news_cls_recm\\pretrained_weights\\bert-base-chinese'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_path)\n",
    "bert_model = BertModel.from_pretrained(bert_path)\n",
    "\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Build Up the Model and Train/Test')\n",
    "    parser.add_argument('--data_path', type=str, default='data', help='Path to the data')\n",
    "    parser.add_argument('--mode', type=str, default='train', help='Mode')\n",
    "    parser.add_argument('--bert', type=str, default='pretrained_weights/bert-base-chinese', help='Path to the data')\n",
    "    parser.add_argument('--max_length', type=int, default=512, help='Maximum length of the sequence')\n",
    "    parser.add_argument('--num_classes', type=int, default=14, help='Number of classes')\n",
    "    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\n",
    "    parser.add_argument('--num_epochs', type=int, default=100, help='Number of epochs')\n",
    "    parser.add_argument('--lr', type=float, default=0.00001, help='Learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=0.01, help='Weight decay')\n",
    "    parser.add_argument('--input_size', type=int, default=768, help='Input size')\n",
    "    parser.add_argument('--hidden_size', type=int, default=256, help='Hidden size')\n",
    "    parser.add_argument('--num_layers', type=int, default=8, help='Number of layers')\n",
    "    parser.add_argument('--num_heads', type=int, default=8, help='Number of heads')\n",
    "    parser.add_argument('--dropout_rate', type=float, default=0.1, help='Dropout rate')\n",
    "    parser.add_argument('--bidirectional', action='store_true', help='Whether to use bidirectional LSTM')    \n",
    "    parser.add_argument('--save_folder', type=str, default='weights', help='Path to save the model')\n",
    "    parser.add_argument('--model_name', type=str, default='bert_naive', help='Name of the model')\n",
    "    parser.add_argument('--device', type=str, default='cuda', help='Device to use')\n",
    "    parser.add_argument('--sample_rate', type=int, default=0.01, help='Number of samples to use')\n",
    "    parser.add_argument('--seed', type=int, default=114514, help='Seed for reproducibility')\n",
    "    parser.add_argument('--continue_training', type=str, default='weights/bert_naive_20240501161942.pth', help='Continue training from a checkpoint')\n",
    "    return parser.parse_args()\n",
    "\"\"\"\n",
    "args = argparse.Namespace(\n",
    "    max_length=512, \n",
    "    num_classes=14, \n",
    "    batch_size=8, \n",
    "    num_epochs=100, \n",
    "    lr=0.00001, \n",
    "    weight_decay=0.01, \n",
    "    input_size=768, \n",
    "    hidden_size=256, \n",
    "    num_layers=8, \n",
    "    num_heads=8, \n",
    "    dropout_rate=0.1, \n",
    "    bidirectional=True, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __author__ = 'Katherine'\n",
    "# from matplotlib.font_manager import FontManager\n",
    "# import subprocess\n",
    "\n",
    "# fm = FontManager()\n",
    "# mat_fonts = set(f.name for f in fm.ttflist)\n",
    "\n",
    "# output = subprocess.check_output(\n",
    "#     'fc-list :lang=zh -f \"%{family}\\n\"', shell=True)\n",
    "# output = output.decode('utf-8')\n",
    "# print '*' * 10, '系统可用的中文字体', '*' * 10\n",
    "# print output\n",
    "# zh_fonts = set(f.split(',', 1)[0] for f in output.split('\\n'))\n",
    "# available = mat_fonts & zh_fonts\n",
    "\n",
    "# print('*' * 10, '可用的字体', '*' * 10)\n",
    "# for f in available:\n",
    "#     print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FontProperties' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#TODO 把这个colormap、label_dict、chinese_font都放到Kernel中\u001b[39;00m\n\u001b[0;32m      2\u001b[0m font \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/usr/share/fonts/MyFonts/simhei.ttf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m chinese_font \u001b[38;5;241m=\u001b[39m \u001b[43mFontProperties\u001b[49m(fname\u001b[38;5;241m=\u001b[39mfont)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin( \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../class_indices.json\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# with open(os.path.join(workplace_root, 'dev/class_indices.json'), 'r') as f:\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FontProperties' is not defined"
     ]
    }
   ],
   "source": [
    "#TODO 把这个colormap、label_dict、chinese_font都放到Kernel中\n",
    "font = '/usr/share/fonts/MyFonts/simhei.ttf'\n",
    "chinese_font = FontProperties(fname=font)\n",
    "\n",
    "try:\n",
    "    with open(os.path.join( '../class_indices.json'), 'r') as f:\n",
    "    # with open(os.path.join(workplace_root, 'dev/class_indices.json'), 'r') as f:\n",
    "        class_indices = {v: k for k,v in json.load(f).items()}\n",
    "    print(class_indices)\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "label_dict = {v: k for k, v in class_indices.items()}\n",
    "print(label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'H:\\大二\\冬季\\创新实训2\\THUCNews\\THUCNews'\n",
    "# data_path = os.path.join(workplace_root, 'THUCNews')\n",
    "random.seed(114514)\n",
    "data_path, data_label = list(read_and_sample_single_folder(data_path, sample_rate=0.01)) # 先采样0.001的数据进行后续测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词云绘制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls = defaultdict(list)\n",
    "for path, label in tqdm(zip(data_path, data_label)):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "    temp = ''\n",
    "    for line in data:\n",
    "        temp += line\n",
    "    data_cls[label].append(temp)\n",
    "print(data_cls)\n",
    "tgt_cls = '体育'\n",
    "wc = WordCloud(\n",
    "    background_color='white',   #背景设置为白色\n",
    "    font_path=font,             #字体\n",
    "    max_words=200,              #最大显示的关键词数量\n",
    "    stopwords=STOPWORDS,        #使用上面导入停用词表\n",
    "    max_font_size=250,          #最大字体\n",
    "    random_state=30,            #设置随机状态数，及配色的方案数\n",
    "    height=860,                 #如果使用默认图片，则可以设置高\n",
    "    margin=2,                   #图片属性\n",
    "    width=1000,                 #设置宽\n",
    ").generate(''.join(data_cls[label_dict[tgt_cls]]))\n",
    "\n",
    "\n",
    "plt.figure(figsize = (20,20), dpi=300)\n",
    "plt.imshow(wc , interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(f'{tgt_cls}词云图',fontsize=100, font=chinese_font)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 defaultdict 用于存储长度计数\n",
    "data_length = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# 遍历每个数据文件并统计文本长度\n",
    "for label, data in data_cls.items():\n",
    "    for text in data:\n",
    "        data_length[label][len(text)] += 1  # 更新对应label和长度的计数器\n",
    "\n",
    "tgt_dict = data_length[label_dict[tgt_cls]]\n",
    "tgt_df = pd.DataFrame(tgt_dict.keys(), index=tgt_dict.values())\n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "sns.histplot(tgt_df, ax=ax, color='teal', kde=True)\n",
    "sns.kdeplot(tgt_df, color='black', ax=ax) \n",
    "plt.title(f'{tgt_cls} 文本长度分布', font=chinese_font)\n",
    "plt.xlabel('文本长度', font=chinese_font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_map = {\n",
    "    bert_model.embeddings(bert_tokenizer(            \n",
    "        text=class_indices[k],\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids']): k for k in data_cls.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = \"篮球比赛\"\n",
    "input_embedding = bert_model.embeddings(bert_tokenizer(            \n",
    "    text=input_,\n",
    "    max_length=512,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt'\n",
    ")['input_ids'])\n",
    "\n",
    "similarities = {}\n",
    "for recommend_vector, cls in rec_map.items():\n",
    "    similarity = torch.nn.functional.cosine_similarity(recommend_vector[0], input_embedding[0])\n",
    "    similarities[cls] = torch.mean(similarity)\n",
    "\n",
    "sorted_recommendations = sorted(similarities.items(), key=lambda x: x[1].item(), reverse=True)\n",
    "\n",
    "top_recommendation = sorted_recommendations[0]\n",
    "\n",
    "print(\"最相似的推荐内容:\", class_indices[top_recommendation[0]])\n",
    "print(\"相似度:\", top_recommendation[1].item())\n",
    "print(\"推荐内容:\\n\", *data_cls[top_recommendation[0]][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cls.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*data_cls[1][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectors = []\n",
    "for cls in tqdm(data_cls.keys()):\n",
    "    texts = data_cls[cls]  \n",
    "    cls_vectors = np.array([np.mean(bert_model.embeddings(bert_tokenizer(            \n",
    "        text=text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )['input_ids']).squeeze(0).detach().numpy(), axis=0) for text in texts])\n",
    "    text_vectors.extend(cls_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.sans-serif']=['SimHei']###解决中文乱码\n",
    "plt.rcParams['axes.unicode_minus']=False\n",
    "\n",
    "\n",
    "text_vectors = np.array(text_vectors)\n",
    "# 现在可以使用K-means进行聚类\n",
    "num_clusters = len(data_cls)  # 假设聚成5类\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "cluster_labels = kmeans.fit_predict(text_vectors)\n",
    "\n",
    "# 使用t-SNE进行降维可视化\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "text_tsne = tsne.fit_transform(text_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = [\n",
    "    'salmon', 'burlywood', 'darkseagreen', 'lightseagreen', 'coral',\n",
    "    'steelblue', 'teal','darkviolet', 'crimson', 'tomato', 'orangered', 'slategray', 'slateblue', 'darkorange', 'gold', 'yellowgreen',\n",
    "    'olivedrab', 'darkgoldenrod', 'sienna', 'peru', 'chocolate', 'saddlebrown',\n",
    "    'firebrick', 'maroon' , 'lightsalmon',\n",
    "    'lightcoral', 'indianred', 'rosybrown', 'sandybrown'\n",
    "]\n",
    "\n",
    "# 绘制聚类可视化结果\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(num_clusters//2):\n",
    "    cluster_points = text_tsne[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'{class_indices[i]}')\n",
    "plt.title('Text Clustering Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制聚类可视化结果\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(num_clusters//2, num_clusters):\n",
    "    cluster_points = text_tsne[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'{class_indices[i]}')\n",
    "plt.title('Text Clustering Visualization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 绘制聚类可视化结果\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(num_clusters):\n",
    "    cluster_points = text_tsne[cluster_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'{class_indices[i]}', c=color_map[i])\n",
    "plt.title('Text Clustering Visualization')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "\n",
    "# 计算轮廓系数\n",
    "silhouette_avg = silhouette_score(text_vectors, cluster_labels)\n",
    "\n",
    "print(f\"The average silhouette_score is : {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, r'H:\\My Code\\nlp\\nlp_course_project-master\\news_cls_recm')\n",
    "\n",
    "from model import Bert_Naive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本功能测试,及实际使用可直接利用的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "那英蛰伏九年十月将发行新专辑 蔡健雅任制作人\n",
    "　　新浪娱乐讯 9年未出唱片的那英(微博)将于10月份发行全新个人专辑《那又怎样》。新专辑由蔡健雅(微博)担任制作人，收录了包括蔡健雅、吴青峰、阿弟仔、小寒等两岸三地以及新加坡、马来西亚的音乐人创作歌曲。下周，那英将赴台进行新专辑封面与音乐录像带的拍摄工作。\n",
    "　　据悉，那英对新专辑《那又怎样》从发想、制作、企划概念都全程亲自参与和执行，整张专辑将呈现那英九年来对生活整理与体验的心得。专辑制作人蔡健雅自言那英是自己的偶像，正是那英的那首《白天不懂夜的黑》开启了她华语音乐的启蒙之门，因此当初那英打电话邀请她担任专辑制作人时，蔡健雅还一度因为担心自己会做不好而想推辞，因为要为自己的偶像制作专辑是个极大的荣幸也是极度的压力，但是那英对蔡健雅充满信心，一直告诉蔡健雅“你可以的，就是你了”。\n",
    "　　此外，专辑的合作团队，那英选择了“亚神音乐”，因为这里有一群曾经与她长期合作且信任的伙伴，那英在台湾的事务也都将交由“亚神音乐”处理。\n",
    "\"\"\"\n",
    "\n",
    "net = Bert_Naive(bert=bert_model, args=args)\n",
    "net.load_state_dict(torch.load('/home/drew/Desktop/nlp_course_project/news_cls_recm/weights/bert_naive_20240501174421.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = bert_tokenizer(text)\n",
    "input_ids, attention_mask = torch.tensor(tokens['input_ids']).unsqueeze(0), torch.tensor(tokens['attention_mask']).unsqueeze(0)\n",
    "print('文本:', text)\n",
    "print('类别:', class_indices[torch.argmax(torch.nn.functional.softmax(net(input_ids, attention_mask), dim=-1)).item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 命名实体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://file.hankcs.com/hanlp/mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip to C:\\Users\\15517\\AppData\\Roaming\\hanlp\\mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip\n",
      "100% 114.3 MiB 547.4 KiB/s ETA:  0 s [=========================================]\n",
      "Decompressing C:\\Users\\15517\\AppData\\Roaming\\hanlp\\mtl/close_tok_pos_ner_srl_dep_sdp_con_electra_small_20210111_124159.zip to C:\\Users\\15517\\AppData\\Roaming\\hanlp\\mtl\n",
      "Downloading https://file.hankcs.com/hanlp/transformers/electra_zh_small_20210706_125427.zip to C:\\Users\\15517\\AppData\\Roaming\\hanlp\\transformers/electra_zh_small_20210706_125427.zip\n",
      "100%  41.2 KiB  41.2 KiB/s ETA:  0 s [=========================================]\n",
      "Decompressing C:\\Users\\15517\\AppData\\Roaming\\hanlp\\transformers/electra_zh_small_20210706_125427.zip to C:\\Users\\15517\\AppData\\Roaming\\hanlp\\transformers\n",
      "Downloading https://file.hankcs.com/corpus/char_table.json.zip to C:\\Users\\15517\\AppData\\Roaming\\hanlp\\thirdparty\\file.hankcs.com\\corpus/char_table.json.zip\n",
      "100%  19.4 KiB  19.4 KiB/s ETA:  0 s [=========================================]\n",
      "Decompressing C:\\Users\\15517\\AppData\\Roaming\\hanlp\\thirdparty\\file.hankcs.com\\corpus/char_table.json.zip to C:\\Users\\15517\\AppData\\Roaming\\hanlp\\thirdparty\\file.hankcs.com\\corpus\n",
      "                                   \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhanlp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m HanLP \u001b[38;5;241m=\u001b[39m hanlp\u001b[38;5;241m.\u001b[39mload(hanlp\u001b[38;5;241m.\u001b[39mpretrained\u001b[38;5;241m.\u001b[39mmtl\u001b[38;5;241m.\u001b[39mCLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) \n\u001b[1;32m----> 3\u001b[0m word\u001b[38;5;241m=\u001b[39mHanLP([\u001b[43mtext\u001b[49m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "import hanlp\n",
    "HanLP = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) \n",
    "word=HanLP([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
